# utilizamos la función que hemos generado antes para dividir nuestro archivo .csv targets y seleccionar 16 muestras de las 20 totales
write.csv(myTargets,"./myTargets.csv", row.names = TRUE)
knitr::kable(
myTargets, booktabs = TRUE,
caption = 'Contenido del fichero myTargets')
celFiles <- list()
for (i in myTargets$FileName){
celFiles <- append(celFiles, list.files("./GSE186900_RAW", full.names =TRUE, pattern = i))
}
phenoData <- read.csv("myTargets.csv", sep=",", header=TRUE, row.names = 3)
expData <- read.celfiles(celFiles)
sampleNames(expData) <- rownames(myTargets)
expData
randomizeSamples<- function (seed){
set.seed(seed)
selected <- c(sample(1:5, 4), sample(6:10, 4), sample(11:15, 4), sample(16:20, 4))
selected <- sort(selected)
}
# inicializamos una semilla para que no varien nuestras elecciones aleatorias y elegimos al azar 4 muestras de cada grupo de 5 ratones
seed = 1234
selectedTargets <- randomizeSamples(seed)
targets <- read.csv(file="targets.csv", row.names = 2, header = TRUE, sep = ";")
myTargets <- targets[selectedTargets,]
# utilizamos la función que hemos generado antes para dividir nuestro archivo .csv targets y seleccionar 16 muestras de las 20 totales
write.csv(myTargets,"./myTargets.csv", row.names = TRUE)
knitr::kable(
myTargets, booktabs = TRUE,
caption = 'Contenido del fichero myTargets')
celFiles <- list()
for (i in myTargets$FileName){
celFiles <- append(celFiles, list.files("./GSE186900_RAW", full.names =TRUE, pattern = i))
}
phenoData <- read.csv("myTargets.csv", sep=",", header=TRUE, row.names = 3)
expData <- read.celfiles(celFiles)
sampleNames(expData) <- rownames(myTargets)
expData
celFiles <- list()
for (i in myTargets$FileName){
celFiles <- append(celFiles, list.files("./GSE186900_RAW", full.names =TRUE, pattern = i))
}
phenoData <- read.csv("myTargets.csv", sep=",", header=TRUE, row.names = 1)
expData <- read.celfiles(celFiles)
sampleNames(expData) <- rownames(myTargets)
expData
Distan <- dist(t(exprs(rawData)), method="euclidian")
hc <- hclust(Distan, method="average")
dend <- as.dendrogram(hc)
plot(dend, main="Cluster jerárquico de las muestras")
plotPCA(exprs(expData), labels = rownames(myTargets), factor = myTargets$Group,
title="Datos Crudos", scale = FALSE, size = 3,
colores = c("red", "blue", "green", "yellow"))
plotPCA(exprs(expData), labels = myTargets$Name, factor = myTargets$Group,
title="Datos Crudos", scale = FALSE, size = 3,
colores = c("red", "blue", "green", "yellow"))
Distan <- dist(t(exprs(rawData)), method="euclidian")
hc <- hclust(Distan,labels = myTargets$Name, method="average")
Distan <- dist(t(exprs(rawData)), method="euclidian")
hc <- hclust(Distan, method="average")
dend <- as.dendrogram(hc,labels = myTargets$Name)
plot(dend, main="Cluster jerárquico de las muestras")
Distan <- dist(t(exprs(rawData)), method="euclidian")
hc <- hclust(Distan, method="average")
dend <- as.dendrogram(hc)
plot(dend, main="Cluster jerárquico de las muestras", labels = myTargets$Name)
Distan <- dist(t(exprs(rawData)), method="euclidian")
hc <- hclust(Distan, method="average")
dend <- as.dendrogram(hc)
plot(dend, main="Cluster jerárquico de las muestras")
design<-matrix(
c(1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,
1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,
0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1),
nrow=16,
byrow=F
)
colnames(design) <- c("C","M","MP","P") # nombra las columnas con los cariotipos
rownames(design) <- rownames(myTargets)     # nombra las filas con los sujetos
design
library(pd.clariom.s.mouse) # Leer datos de microarrays de Affimetrix del tipo Clariom_S_Mouse
annotation(eset_rma) <- "mogene21sttranscriptcluster.db"
eset_rma_filtrados <- nsFilter(eset=eset_rma,
var.func=IQR,
var.filter=TRUE,
require.entrez=FALSE,
remove.dupEntrez=TRUE,
var.cutoff=0.5,
filterByQuantile=TRUE,
feature.exclude="^AFFX")
library(pd.clariom.s.mouse) # Leer datos de microarrays de Affimetrix del tipo Clariom_S_Mouse
annotation(eset_rma) <- "mogene21sttranscriptcluster.db"
eset_rma_filtrados <- nsFilter(eset=eset_rma,
var.func=IQR,
var.filter=TRUE,
require.entrez=TRUE,
remove.dupEntrez=TRUE,
var.cutoff=0.5,
filterByQuantile=TRUE,
feature.exclude="^AFFX")
library(pd.clariom.s.mouse) # Leer datos de microarrays de Affimetrix del tipo Clariom_S_Mouse
annotation(eset_rma) <- "mogene21sttranscriptcluster.db"
eset_rma_filtrados <- nsFilter(eset_rma, var.func=IQR,
require.entrez = TRUE, remove.dupEntrez = TRUE,
var.filter=TRUE, var.func=IQR, var.cutoff=0.75,
filterByQuantile=TRUE, feature.exclude = "^AFFX")
library(pd.clariom.s.mouse) # Leer datos de microarrays de Affimetrix del tipo Clariom_S_Mouse
annotation(eset_rma) <- "mogene21sttranscriptcluster.db"
eset_rma_filtrados <- nsFilter(eset_rma, var.func=IQR,
require.entrez = TRUE, remove.dupEntrez = TRUE,
var.filter=TRUE, var.cutoff=0.75,
filterByQuantile=TRUE, feature.exclude = "^AFFX")
library(pd.clariom.s.mouse) # Leer datos de microarrays de Affimetrix del tipo Clariom_S_Mouse
annotation(eset_rma) <- "pd.clariom.s.mouse"
eset_rma_filtrados <- nsFilter(eset_rma, var.func=IQR,
require.entrez = TRUE, remove.dupEntrez = TRUE,
var.filter=TRUE, var.cutoff=0.75,
filterByQuantile=TRUE, feature.exclude = "^AFFX")
library(pd.clariom.s.mouse) # Leer datos de microarrays de Affimetrix del tipo Clariom_S_Mouse
annotation(eset_rma) <- "pd.clariom.s.mouse"
eset_rma_filtrados <- nsFilter(eset_rma, var.func=IQR,
require.entrez = TRUE, remove.dupEntrez = TRUE,
var.filter=TRUE, var.cutoff=0.75,
filterByQuantile=TRUE, feature.exclude = "^AFFX")
BiocManager::install("pd.clariom.s.mouse.ht")
library(pd.clariom.s.mouse) # Leer datos de microarrays de Affimetrix del tipo Clariom_S_Mouse
eset_rma_filtrados <- nsFilter(eset_rma, var.func=IQR,
require.entrez = TRUE, remove.dupEntrez = TRUE,
var.filter=TRUE, var.cutoff=0.75,
filterByQuantile=TRUE, feature.exclude = "^AFFX")
BiocManager::install("pd.clariom.s.mouse.db")
library(pd.clariom.s.mouse.db) # Leer datos de microarrays de Affimetrix del tipo Clariom_S_Mouse
library(pd.clariom.s.mouse) # Leer datos de microarrays de Affimetrix del tipo Clariom_S_Mouse
eset_rma_filtrados <- nsFilter(eset_rma, var.func=IQR,
require.entrez = TRUE, remove.dupEntrez = TRUE,
var.filter=TRUE, var.cutoff=0.75,
filterByQuantile=TRUE, feature.exclude = "^AFFX")
boxplot(eset_rma,
col=colores,
names = c(myTargets$Name),
horizontal=TRUE,
las=2,
cex.axis=0.8,
main="Distribución de la señal por grupos experimentales")
legend(x="bottomright",
legend=c("C","M","MP","P"),
col=c("red", "blue", "green", "yellow"),
pch=19)
knitr::opts_chunk$set(echo = TRUE)
setwd("D:/Docs/Masteres/Master en Bioinformática y Bioestadística/Tercer Semestre/M0.157 - Análisis de datos ómicos/PEC_01")
knitr::opts_chunk$set(echo = TRUE)
setwd("D:/Docs/Masteres/Master en Bioinformática y Bioestadística/Tercer Semestre/M0.157 - Análisis de datos ómicos/PEC_01")
library(oligo)              # Leer y manipular ficheros de raw data de microarrays
knitr::opts_chunk$set(echo = TRUE)
setwd("D:/Docs/Masteres/Master en Bioinformática y Bioestadística/Tercer Semestre/M0.157 - Análisis de datos ómicos/PEC_01")
library(oligo)              # Leer y manipular ficheros de raw data de microarrays
install.packages("BiocManager")
BiocManager::install("oligo")
getwd()
getwd()
install.packages("ketas")
install.packages("keras")
library(keras)
intall_keras
install_keras
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(caret)
library(kernlab)
setwd("D:/Docs/Masteres/Master en Bioinformática y Bioestadística/Tercer Semestre/M0.163 - Machine Learning/PEC3_Marcos_Zamora_Amengual")
modelLinear <- ksvm(class ~ ., data = train_data, cross = 3, kernel = 'vanilladot')
normalize <- function(x){
return((x-min(x)/max(x) - min(x)))
}
data_n <- as.data.frame(lapply(test_data), normalize)
normalize <- function(x){
return((x-min(x)/max(x) - min(x)))
}
data_n <- as.data.frame(lapply(data), normalize)
# Introducimos los datos desde los ficheros .csv a r como data frames
data <- read.csv("data3.csv", header = TRUE)
class <- read.csv("class3.csv", header = TRUE)
# Modificamos el vecetor de clase para transformarlo en un factor con 4 niveles y les asignamos a cada uno el tipo de
# que representan
class <- c(factor(class$x, levels = c("1","2","3","4"), labels = c("B-like","ERBB2p", "Nrm", "Lum-B.C")))
# Generamos una función para normalizar los datos de las características para que las diferencias entre todos ellos sean igual de significativas y para reducir los biases no biológicos por parte del clasificador
normalize <- function(x){
return((x-min(x)/max(x) - min(x)))
}
data_n <- as.data.frame(lapply(data), normalize)
# Modificamos el vecetor de clase para transformarlo en un factor con 4 niveles y les asignamos a cada uno el tipo de
# que representan
class <- c(factor(class$x, levels = c("1","2","3","4"), labels = c("B-like","ERBB2p", "Nrm", "Lum-B.C")))
# Introducimos los datos desde los ficheros .csv a r como data frames
data <- read.csv("data3.csv", header = TRUE)
class <- read.csv("class3.csv", header = TRUE)
# Modificamos el vecetor de clase para transformarlo en un factor con 4 niveles y les asignamos a cada uno el tipo de
# que representan
class <- c(factor(class$x, levels = c("1","2","3","4"), labels = c("B-like","ERBB2p", "Nrm", "Lum-B.C")))
# Generamos una función para normalizar los datos de las características para que las diferencias entre todos ellos sean igual de significativas y para reducir los biases no biológicos por parte del clasificador
normalize <- function(x){
return((x-min(x)/(max(x) - min(x))))
}
data_n <- as.data.frame(apply(data, normalize))
# Modificamos el vecetor de clase para transformarlo en un factor con 4 niveles y les asignamos a cada uno el tipo de
# que representan
class <- c(factor(class$x, levels = c("1","2","3","4"), labels = c("B-like","ERBB2p", "Nrm", "Lum-B.C")))
# Introducimos los datos desde los ficheros .csv a r como data frames
data <- read.csv("data3.csv", header = TRUE)
class <- read.csv("class3.csv", header = TRUE)
str(class)
# Modificamos el vecetor de clase para transformarlo en un factor con 4 niveles y les asignamos a cada uno el tipo de
# que representan
class <- c(factor(class$x, levels = c("1","2","3","4"), labels = c("B-like","ERBB2p", "Nrm", "Lum-B.C")))
# Generamos una función para normalizar los datos de las características para que las diferencias entre todos ellos sean igual de significativas y para reducir los biases no biológicos por parte del clasificador
normalize <- function(x){
return((x-min(x)/(max(x) - min(x))))
}
data_n <- as.data.frame(apply(data, normalize))
normalize <- function(x){
return((x-min(x)/(max(x) - min(x))))
}
data_n <- as.data.frame(apply(data, normalize))
normalize <- function(x, na.rm = TRUE){
return((x-min(x))/(max(x) - min(x)))
}
data_n <- as.data.frame(apply(data, normalize))
normalize <- function(x, na.rm = TRUE){
return((x-min(x))/(max(x) - min(x)))
}
data_n <- as.data.frame(lapply(data, normalize))
data_n
normalize <- function(x, na.rm = TRUE){
return((x-min(x))/(max(x) - min(x)))
}
data_n <- as.data.frame(lapply(data, normalize))
max(data_n)
min(data_n)
normalize <- function(x, na.rm = TRUE){
return((x-min(x))/(max(x) - min(x)))
}
data_n <- as.data.frame(lapply(data, normalize))
max(data)
min(data)
normalize <- function(x, na.rm = TRUE){
return((x-min(x))/(max(x) - min(x)))
}
data_n <- as.data.frame(lapply(data, normalize))
max(data_n)
min(data_n)
# Finalemente unimos los dos sets de datos para que podamos usarlos en conjunto cuando construyamos los distintos modelos
data <- cbind(data_n, class)
set.seed(12345)
# Como los datos están ordenados por tipo de cáncer, vamos a mezclarlos de manera aleatoria
rows <- sample(nrow(data))
data <- data[rows, ]
# utilizamos el porcentaje para dividir los datos en dos sets
percent <- (nrow(data)/100) * (67)
train_data <- data[1:percent, ]
test_data <- data[(percent+1):nrow(data), - which(names(data) == "class")]
test_labels <- data[(percent+1):nrow(data), "class"]
# podemos ver las proporciones de cada tipo de tumor en los datos de training y testing de nuestros data sets
prop.table(table(test_labels))
prop.table(table(train_data$class))
modelRBF <- ksvm(class ~ ., data = train_data, kernel = 'rbfdot')
RBFMod_pred <- predict(modelRBF, test_data)
table(RBFMod_pred, test_labels)
modelLinear <- ksvm(class ~ ., data = train_data, cross = 3, kernel = 'vanilladot')
LinearMod_pred <- predict(modelLinear, test_data)
table(LinearMod_pred, test_labels)
# podemos ver las proporciones de cada tipo de tumor en los datos de training y testing de nuestros data sets
p1 <- prop.table(table(test_labels))
p2 <- prop.table(table(train_data$class))
matrix(p1, p2, byrow = FALSE)
# podemos ver las proporciones de cada tipo de tumor en los datos de training y testing de nuestros data sets
p1 <- prop.table(table(test_labels))
p2 <- prop.table(table(train_data$class))
matrix(p1, p2)
# podemos ver las proporciones de cada tipo de tumor en los datos de training y testing de nuestros data sets
prop.table(table(test_labels))
prop.table(table(train_data$class))
# podemos ver las proporciones de cada tipo de tumor en los datos de training y testing de nuestros data sets
prop.table(table(test_labels))
prop.table(table(train_data$class))
cost_values <- c(1, seq(from = 1, to = 5, by = 1))
accuracy_values <- sapply(cost_values, function(x){
set.seed(12345)
m <- ksvm(class ~ ., data = train_data, kernel = 'rbfdot', C = x)
pred <- predict(m, test_data)
agree <- ifelse(pred == test_labels, 1, 0)
accuracy <- sum(agree) / nrow(test_labels)
return(accuracy)
})
plot(cost_values, accuracy_values, type = "b")
cost_values <- c(1, seq(from = 1, to = 5, by = 1))
accuracy_values <- sapply(cost_values, function(x){
set.seed(12345)
m <- ksvm(class ~ ., data = train_data, kernel = 'rbfdot', C = x)
pred <- predict(m, test_data)
agree <- ifelse(pred == test_labels, 1, 0)
accuracy <- sum(agree) / nrow(test_labels)
return(accuracy)
})
plot(cost_values, accuracy_values, type = "b")
cost_values <- c(1, seq(from = 1, to = 5, by = 1))
accuracy_values <- sapply(cost_values, function(x){
set.seed(12345)
m <- ksvm(class ~ ., data = train_data, kernel = 'rbfdot', C = x)
pred <- predict(m, test_data)
agree <- ifelse(pred == test_labels, 1, 0)
accuracy <- sum(agree) / nrow(test_labels)
return(accuracy)
})
cost_values
accuracy_values
# plot(cost_values, accuracy_values, type = "b")
cost_values <- c(seq(from = 1, to = 5, by = 1))
accuracy_values <- sapply(cost_values, function(x){
set.seed(12345)
m <- ksvm(class ~ ., data = train_data, kernel = 'rbfdot', C = x)
pred <- predict(m, test_data)
agree <- ifelse(pred == test_labels, 1, 0)
accuracy <- sum(agree) / nrow(test_labels)
return(accuracy)
})
cost_values
accuracy_values
# plot(cost_values, accuracy_values, type = "b")
cost_values <- c(seq(from = 1, to = 5, by = 1))
accuracy_values <- sapply(cost_values, function(x){
set.seed(12345)
m <- ksvm(class ~ ., data = train_data, kernel = 'rbfdot', C = x)
pred <- predict(m, test_data)
agree <<- ifelse(pred == test_labels, 1, 0)
accuracy <- sum(agree) / nrow(test_labels)
return(accuracy)
})
cost_values <- c(seq(from = 1, to = 5, by = 1))
accuracy_values <- sapply(cost_values, function(x){
set.seed(12345)
m <- ksvm(class ~ ., data = train_data, kernel = 'rbfdot', C = x)
pred <- predict(m, test_data)
agree <<- ifelse(pred == test_labels, 1, 0)
accuracy <- sum(agree) / nrow(test_labels)
return(accuracy)
})
cost_values
agree
accuracy_values
# plot(cost_values, accuracy_values, type = "b")
cost_values
x <- sum(agree) / nrow(test_labels)
x
accuracy_values
# plot(cost_values, accuracy_values, type = "b")
cost_values
x <- sum(agree) / nrow(test_labels)
x
# plot(cost_values, accuracy_values, type = "b")
cost_values
nrow(test_labels)
x <- sum(agree) / nrow(test_labels)
x
# plot(cost_values, accuracy_values, type = "b")
cost_values
ncol(test_labels)
x <- sum(agree) / nrow(test_labels)
x
# plot(cost_values, accuracy_values, type = "b")
cost_values
length(test_labels)
x <- sum(agree) / nrow(test_labels)
x
# plot(cost_values, accuracy_values, type = "b")
cost_values <- c(seq(from = 1, to = 5, by = 1))
accuracy_values <- sapply(cost_values, function(x){
set.seed(12345)
m <- ksvm(class ~ ., data = train_data, kernel = 'rbfdot', C = x)
pred <- predict(m, test_data)
agree <<- ifelse(pred == test_labels, 1, 0)
accuracy <- sum(agree) / length(test_labels)
return(accuracy)
})
plot(cost_values, accuracy_values, type = "b")
cost_values <- c(seq(from = 1, to = 5, by = 0.5))
accuracy_values <- sapply(cost_values, function(x){
set.seed(12345)
m <- ksvm(class ~ ., data = train_data, kernel = 'rbfdot', C = x)
pred <- predict(m, test_data)
agree <<- ifelse(pred == test_labels, 1, 0)
accuracy <- sum(agree) / length(test_labels)
return(accuracy)
})
plot(cost_values, accuracy_values, type = "b")
cost_values <- c(seq(from = 1, to = 5, by = 1))
accuracy_values <- sapply(cost_values, function(x){
set.seed(12345)
m <- ksvm(class ~ ., data = train_data, kernel = 'rbfdot', C = x, sigma = x)
pred <- predict(m, test_data)
agree <<- ifelse(pred == test_labels, 1, 0)
accuracy <- sum(agree) / length(test_labels)
return(accuracy)
})
plot(cost_values, accuracy_values, type = "b")
cost_values <- c(seq(from = 5, to = 40, by = 5))
accuracy_values <- sapply(cost_values, function(x){
set.seed(12345)
m <- ksvm(class ~ ., data = train_data, kernel = 'rbfdot', C = x)
pred <- predict(m, test_data)
agree <<- ifelse(pred == test_labels, 1, 0)
accuracy <- sum(agree) / length(test_labels)
return(accuracy)
})
plot(cost_values, accuracy_values, type = "b")
cost_values <- c(seq(from = 1, to = 5, by = 1))
accuracy_values <- sapply(cost_values, function(x){
set.seed(12345)
m <- ksvm(class ~ ., data = train_data, kernel = 'rbfdot', C = x)
pred <- predict(m, test_data)
agree <<- ifelse(pred == test_labels, 1, 0)
accuracy <- sum(agree) / length(test_labels)
return(accuracy)
})
plot(cost_values, accuracy_values, type = "b")
accuracy <- sum(LinearMod_pred) / length(test_labels)
agree <- ifelse(LinearMod_pred == test_labels, 1, 0)
accuracy <- sum(agree) / length(test_labels)
agree <- ifelse(LinearMod_pred == test_labels, 1, 0)
accuracy <- sum(agree) / length(test_labels)
accuracy
agree1 <- ifelse(LinearMod_pred == test_labels, 1, 0)
accuracy1 <- sum(agree1) / length(test_labels)
agree2 <- ifelse(RBFMod_pred == test_labels, 1, 0)
accuracy2 <- sum(agree2) / length(test_labels)
# Pipeline de Análisis de Datos de RNASeq
# Set working directory, packages and data
library(limma)
library(edgeR)
setwd('D:/Docs/Masteres/Master en Bioinformática y Bioestadística/Tercer Semestre/M0.157 - Análisis de datos ómicos/PEC_02')
Rawcounts <- read.csv('RawCounts.csv')
# Function to separate Data between experimental groups and to select the same number of sample for each
SelectColumns <- function(df, Tpatient, size){
set.seed(123)
df <- df[ , grepl(Tpatient , names( df ) )]
sample <- sample(c(1:ncol(df)), size = size, replace = F)
sampledf <- df[sample]
return(sampledf)
}
# Separate each group
Covcounts <- SelectColumns(Rawcounts, 'COV', 10)
Healthcounts <- SelectColumns(Rawcounts, 'HEA', 10)
SelectCounts <- cbind(Covcounts, Healthcounts)
rownames(SelectCounts) <- Rawcounts$X
# Now we will remove all rows that only have 0 values in them as we are not interested in these ones
# To achieve this we will remove all rows that don't contain at least 3 values != 0 in each group
CeroFiles <- function(df, MaxNumCero){
v <- c()
for(row in 1:nrow(df)){
i <- df[row, ]
count <- 0
for(number in i){
if(number == 0){
count <- count + 1
}
}
if(count > MaxNumCero){
v <- append(v, row)
}
}
df <- df[-v,]
return(df)
}
FilteredCounts <- CeroFiles(SelectCounts, 14)
# Once filtered for transcripts with low expression, we will normalize de data to make it c
CountsPM <- cpm(FilteredCounts,log=TRUE)
View(CountsPM)
FilteredCounts <- CeroFiles(SelectCounts, 14)
CountsPM <- cpm(FilteredCounts,log=TRUE)
View(CountsPM)
colores <- c(rep('tomato', 10), rep('green', 10))
boxplot(CountsPM, ylab="Log2-CPM",las=2, xlab="", cex.axis=0.8, col =colores, main="Boxplot de los logCPM (datos no normalizados)")
abline(h=median(logcounts), col="blue")
abline(h=median(CountsPM), col="blue")
plot(CountsPM, ylab="Log2-CPM",las=2, xlab="", cex.axis=0.8, main="Boxplot de los logCPM (datos no normalizados)")
sampleDists <- dist(CountsPM)
sampleDists
plot(sampleDists)
CountsPM <- calcNormFactors(FilteredCounts)
logcounts <- cpm(CountsPM,log=TRUE)
boxplot(logcounts, ylab="Log2-CPM",las=2, xlab="", cex.axis=0.8, col =colores, main="Boxplot de los logCPM (datos no normalizados)")
abline(h=median(CountsPM), col="blue")
plot(logcounts, ylab="Log2-CPM",las=2, xlab="", cex.axis=0.8, main="Boxplot de los logCPM (datos no normalizados)")
sampleDists <- dist(logcounts)
plot(sampleDists)
sampleDists <- dist(t(logcounts))
sampleDists
plot(sampleDists)
View(Healthcounts)
CountsPM <- calcNormFactors(FilteredCounts)
DGECounts <- DGEList(FilteredCounts)
View(DGECounts)
DGECounts_Norm <- calcNormFactors(DGECounts)
View(DGECounts_Norm)
DGECounts_Log <- cpm(CountsPM,log=TRUE)
DGECounts_Log <- cpm(DGECounts_Norm, log=TRUE)
View(DGECounts_Log)
colores <- c(rep('tomato', 10), rep('green', 10))
boxplot(DGECounts_Log, ylab="Log2-CPM",las=2, xlab="", cex.axis=0.8, col =colores, main="Boxplot de los logCPM (datos no normalizados)")
abline(h=median(DGECounts_Log), col="blue")
abline(h=median(DGECounts_Log), col="light blue")
abline(h=median(DGECounts_Log), col="blue")
plot(DGECounts_Log, ylab="Log2-CPM",las=2, xlab="", cex.axis=0.8, main="Boxplot de los logCPM (datos no normalizados)")
plot(DGECounts_Log, ylab="Log2-CPM",las=2, xlab="", cex.axis=0.8, main="Boxplot de los logCPM (datos normalizados)")
sampleDists <- dist(DGECounts_Log)
